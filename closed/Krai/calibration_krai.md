# MLPerf Inference v3.1 - KRAI - Calibration Details

For results obtained using KRAI Inference Library Technology (KILT), we prepare quantized models using vendor-provided tools.

## NVIDIA TensorRT

We prepare [quantized TensorRT plans](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html) following the procedure outlined in [NVIDIA's MLPerf Inference v3.0 submission](https://github.com/mlcommons/inference_results_v3.0/blob/master/closed/NVIDIA/documentation/calibration.md).

## QUALCOMM Snapdragon Neural Processing Engine (SNPE)

We prepare [quantized DLC models](https://developer.qualcomm.com/sites/default/files/docs/snpe/quantized_models.html) following the procedure outlined in [QUALCOMM's model conversion documentation](https://developer.qualcomm.com/sites/default/files/docs/snpe/model_conversion.html).
