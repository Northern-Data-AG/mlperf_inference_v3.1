From 2adaa427758140e81ea9d98310c75043a5590d24 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Micha=C5=82=20Zientkiewicz?= <michalz@nvidia.com>
Date: Tue, 9 May 2023 13:00:09 +0200
Subject: [PATCH] Improve VA reservation robustness

---
 dali/core/mm/cuda_vm_resource_test.cc   | 100 +++++++-
 dali/core/mm/default_resource_test.cc   | 222 ++++++++++++++++++
 include/dali/core/mm/cuda_vm_resource.h | 292 ++++++++++++++++++++++--
 3 files changed, 592 insertions(+), 22 deletions(-)

diff --git a/dali/core/mm/cuda_vm_resource_test.cc b/dali/core/mm/cuda_vm_resource_test.cc
index 5efeac06..8630876e 100644
--- a/dali/core/mm/cuda_vm_resource_test.cc
+++ b/dali/core/mm/cuda_vm_resource_test.cc
@@ -116,11 +116,11 @@ class VMResourceTest : public ::testing::Test {
     cuvm::CUAddressRange total = res.va_ranges_.back();
     cuvm::CUAddressRange part1 = { total.ptr(),           s1 };
     cuvm::CUAddressRange part2 = { total.ptr() + s1,      s2 };
-    res.va_add_region(part1);
+    res.va_add_range(part1);
     ASSERT_EQ(res.va_regions_.size(), 1u);
     MapRandomBlocks(res.va_regions_[0], 10);
     va_region_backup va1 = Backup(res.va_regions_[0]);
-    res.va_add_region(part2);
+    res.va_add_range(part2);
     ASSERT_EQ(res.va_regions_.size(), 1u);
     auto &region = res.va_regions_.back();
     ASSERT_EQ(region.num_blocks(), b1 + b2);
@@ -138,11 +138,11 @@ class VMResourceTest : public ::testing::Test {
     cuvm::CUAddressRange total = res.va_ranges_.back();
     cuvm::CUAddressRange part1 = { total.ptr(),           s1 };
     cuvm::CUAddressRange part2 = { total.ptr() + s1,      s2 };
-    res.va_add_region(part2);
+    res.va_add_range(part2);
     ASSERT_EQ(res.va_regions_.size(), 1u);
     MapRandomBlocks(res.va_regions_[0], 10);
     va_region_backup va1 = Backup(res.va_regions_[0]);
-    res.va_add_region(part1);
+    res.va_add_range(part1);
     ASSERT_EQ(res.va_regions_.size(), 1u);
     auto &region = res.va_regions_.back();
     ASSERT_EQ(region.num_blocks(), b1 + b2);
@@ -162,15 +162,15 @@ class VMResourceTest : public ::testing::Test {
     cuvm::CUAddressRange part1 = { total.ptr(),           s1 };
     cuvm::CUAddressRange part2 = { total.ptr() + s1,      s2 };
     cuvm::CUAddressRange part3 = { total.ptr() + s1 + s2, s3 };
-    res.va_add_region(part1);
+    res.va_add_range(part1);
     ASSERT_EQ(res.va_regions_.size(), 1u);
     MapRandomBlocks(res.va_regions_[0], 10);
     va_region_backup va1 = Backup(res.va_regions_[0]);
-    res.va_add_region(part3);
+    res.va_add_range(part3);
     ASSERT_EQ(res.va_regions_.size(), 2u);
     MapRandomBlocks(res.va_regions_[1], 12);
     va_region_backup va3 = Backup(res.va_regions_[1]);
-    res.va_add_region(part2);
+    res.va_add_range(part2);
     ASSERT_EQ(res.va_regions_.size(), 1u);
     auto &region = res.va_regions_.back();
     ASSERT_EQ(region.num_blocks(), b1 + b2 + b3);
@@ -208,6 +208,80 @@ class VMResourceTest : public ::testing::Test {
     EXPECT_EQ(region.available_blocks, 3);
   }
 
+
+  void TestReleaseUnused() {
+    cuda_vm_resource res;
+    size_t block_size = 4 << 20;  // 4 MiB;
+    res.block_size_ = block_size;
+    void *p1 = res.allocate(block_size / 2);    // allocate half block
+    void *p2 = res.allocate(block_size / 2);    // allocate another half of a block
+    void *p3 = res.allocate(block_size);        // allocate another block
+    auto &region = res.va_regions_[0];
+    EXPECT_EQ(res.stat_.allocated_blocks, 2);
+    res.deallocate(p1, block_size / 2);         // now free the first half-block
+    EXPECT_EQ(region.available_blocks, 0);      // only half block was freed
+    res.release_unused();
+    EXPECT_EQ(res.stat_.total_unmaps, 0);       // cannot unmap a partially occupied block
+
+    res.deallocate(p2, block_size / 2);         // free the other half of the block
+
+    EXPECT_EQ(region.available_blocks, 1);
+    EXPECT_EQ(region.mapped.find(false), 2);    // 2 mapped blocks
+    EXPECT_EQ(region.available.find(true), 0);  // of which the first is available
+    res.release_unused();
+    EXPECT_EQ(res.stat_.total_unmaps, 1);       // the 1st block should be unmapped
+    EXPECT_EQ(region.available_blocks, 0);      // after unmapping, it shouldn't be available
+    EXPECT_EQ(region.mapped.find(false), 0);    // the block was unmapped
+    EXPECT_EQ(region.mapped.find(true), 1);     // but the next one wasn't
+    EXPECT_EQ(res.stat_.allocated_blocks, 1);
+    EXPECT_EQ(res.stat_.peak_allocated_blocks, 2);
+
+    void *p4 = res.allocate(block_size);  // allocate one block
+    EXPECT_EQ(res.stat_.allocated_blocks, 2);
+
+    EXPECT_EQ(p1, p4);
+    res.deallocate(p4, block_size);
+    res.deallocate(p3, block_size);
+    EXPECT_EQ(region.available_blocks, 2);
+    res.release_unused();
+    EXPECT_EQ(region.available_blocks, 0);
+    EXPECT_EQ(region.mapped.find(true), region.mapped.ssize());  // no mapped blocks
+    EXPECT_EQ(res.stat_.allocated_blocks, 0);
+    EXPECT_EQ(res.stat_.peak_allocated_blocks, 2);
+  }
+
+  void TestReleaseUnusedVA() {
+    cuda_vm_resource res;
+    size_t block_size = 4 << 20;  // 4 MiB;
+    size_t alloc_size = 4 * block_size;
+    res.block_size_ = block_size;
+    res.initial_va_size_ = alloc_size;
+    // 1st VA allocation
+    void *p0 = res.allocate(alloc_size);
+
+    // 2nd VA allocation
+    void *p1 = res.allocate(alloc_size);
+    void *p2 = res.allocate(alloc_size);
+
+    // 3rd VA allocation
+    void *p3 = res.allocate(4 * alloc_size);
+
+    EXPECT_EQ(res.stat_.peak_va, 7 * alloc_size);
+    EXPECT_EQ(res.stat_.allocated_va, 7 * alloc_size);
+
+    // free half of the 2nd VA allocation
+    res.deallocate(p1, alloc_size);
+    EXPECT_EQ(0, res.release_unused_va());
+
+    // free the rest of the 2nd VA allocation
+    res.deallocate(p2, alloc_size);
+
+    auto va_freed = res.release_unused_va();
+    EXPECT_EQ(va_freed, 2 * alloc_size);
+    EXPECT_EQ(res.stat_.total_unmaps, 8);
+    EXPECT_EQ(res.stat_.allocated_va, 5 * alloc_size);
+  }
+
   void TestExceptionSafety() {
     cudaDeviceProp device_prop;
     CUDA_CALL(cudaGetDeviceProperties(&device_prop, 0));
@@ -291,6 +365,18 @@ TEST_F(VMResourceTest, PartialMap) {
   this->TestPartialMap();
 }
 
+TEST_F(VMResourceTest, ReleaseUnused) {
+  if (!cuvm::IsSupported())
+    GTEST_SKIP() << "CUDA Virtual Memory Management not supported on this platform";
+  this->TestReleaseUnused();
+}
+
+TEST_F(VMResourceTest, ReleaseUnusedVA) {
+  if (!cuvm::IsSupported())
+    GTEST_SKIP() << "CUDA Virtual Memory Management not supported on this platform";
+  this->TestReleaseUnusedVA();
+}
+
 std::string format_size(size_t bytes) {
   std::stringstream ss;
   print(ss, bytes, " bytes");
diff --git a/dali/core/mm/default_resource_test.cc b/dali/core/mm/default_resource_test.cc
index 5e25d0d1..3b83abd4 100644
--- a/dali/core/mm/default_resource_test.cc
+++ b/dali/core/mm/default_resource_test.cc
@@ -295,6 +295,228 @@ TEST(MMDefaultResource, GetResource_Device_RangeCheck_MultiGPU) {
   EXPECT_THROW(GetDefaultDeviceResource(ndev+100), std::out_of_range);
 }
 
+inline bool UseVMM() {
+  static const bool use_vmm = []() {
+    auto *res = mm::GetDefaultDeviceResource();
+    if (auto *up = dynamic_cast<mm::with_upstream<mm::memory_kind::device> *>(res)) {
+      return dynamic_cast<mm::cuda_vm_resource*>(up->upstream()) != nullptr;
+    }
+    return false;
+  }();
+  return use_vmm;
+}
+
+template <typename Kind>
+mm::pool_resource_base<Kind> *GetPoolInterface(mm::memory_resource<Kind> *mr) {
+  while (mr) {
+    if (auto *pool = dynamic_cast<mm::pool_resource_base<Kind>*>(mr))
+      return pool;
+    if (auto *up = dynamic_cast<mm::with_upstream<Kind>*>(mr)) {
+      mr = up->upstream();
+    } else {
+      break;
+    }
+  }
+  return nullptr;
+}
+
+static mm::cuda_vm_resource *GetVMMDefaultResource(int device_id = -1) {
+  auto *res = mm::GetDefaultDeviceResource(device_id);
+  if (auto *up = dynamic_cast<mm::with_upstream<mm::memory_kind::device> *>(res)) {
+    return dynamic_cast<mm::cuda_vm_resource*>(up->upstream());
+  }
+  return nullptr;
+}
+
+
+static void ReleaseUnusedTestImpl(ssize_t max_alloc_size = std::numeric_limits<ssize_t>::max()) {
+  auto *dev = mm::GetDefaultDeviceResource(0);
+  auto *pinned = mm::GetDefaultResource<mm::memory_kind::pinned>();
+
+  CUDA_CALL(cudaDeviceSynchronize());
+  mm::ReleaseUnusedMemory();
+
+  size_t free0 = 0;  // before allocation
+  size_t free1 = 0;  // after allocation
+  size_t free2 = 0;  // ReleaseUnused called before deallocation
+  size_t free3 = 0;  // ReleaseUnused called after deallocation
+  size_t total = 0;
+
+  CUDA_CALL(cudaMemGetInfo(&free0, &total));
+  ssize_t min_dev_size = 256;
+  ssize_t dev_size = std::min<ssize_t>(free0 - (64_z << 20), max_alloc_size);
+  ssize_t pinned_size = 256_z << 20;  // 256 MiB
+  ASSERT_GE(dev_size, min_dev_size);
+
+  mm::uptr<void> mem_dev;
+  while (dev_size >= min_dev_size) {
+    try {
+      mem_dev = mm::alloc_raw_unique<char>(dev, dev_size);
+      break;
+    } catch (const std::bad_alloc &) {
+      dev_size >>= 1;
+    }
+  }
+  ASSERT_NE(mem_dev, nullptr) << "Couldn't allocate any device memory - cannot continue testing";
+
+  mm::uptr<void> mem_pinned = mm::alloc_raw_unique<char>(pinned, pinned_size);
+  CUDA_CALL(cudaMemGetInfo(&free1, &total));
+
+  mm::ReleaseUnusedMemory();
+  CUDA_CALL(cudaMemGetInfo(&free2, &total));
+  EXPECT_EQ(free2, free1) << "Nothing should have been released.";
+
+  mem_dev.reset();
+  mem_pinned.reset();
+
+  mm::ReleaseUnusedMemory();
+  CUDA_CALL(cudaMemGetInfo(&free3, &total));
+  EXPECT_GT(free3, free2);
+}
+
+TEST(MMDefaultResource, ReleaseUnusedBasic) {
+  cudaDeviceProp device_prop;
+  CUDA_CALL(cudaGetDeviceProperties(&device_prop, 0));
+  if (device_prop.integrated)
+    GTEST_SKIP() << "The memory usage on integrated GPUs cannot be reliably tracked.";
+
+  ReleaseUnusedTestImpl(256 << 20);
+}
+
+TEST(MMDefaultResource, ReleaseUnusedMaxMem) {
+  if (!UseVMM())
+    GTEST_SKIP() << "Cannot reliably test ReleaseUnused with max mem usage without VMM support";
+
+  cudaDeviceProp device_prop;
+  CUDA_CALL(cudaGetDeviceProperties(&device_prop, 0));
+  if (device_prop.integrated)
+    GTEST_SKIP() << "The memory usage on integrated GPUs cannot be reliably tracked.";
+
+  ReleaseUnusedTestImpl();
+}
+
+// This can be run manually - it can still work, but it's not reliable when run
+// alongside other tests.
+TEST(MMDefaultResource, DISABLED_ReleaseUnusedMaxMem) {
+  ReleaseUnusedTestImpl();
+}
+
+TEST(MMDefaultResource, PreallocatePinnedMemory) {
+  mm::ReleaseUnusedMemory();  // release any unused memory to check that we're really preallocating
+
+  auto *res = mm::GetDefaultResource<mm::memory_kind::pinned>();
+  auto *pool = GetPoolInterface(res);
+  if (!pool)
+    GTEST_SKIP() << "No memory pool in use - cannot test pool preallocation.";
+
+  size_t size = 64_uz << 20;  // 64 MiB
+  size_t alignment = alignof(std::max_align_t);  // default alignment
+  // Try to get increasing amount of memory from the pool until we can't
+  for (;; size <<= 1) {
+    void *mem = pool->try_allocate_from_free(size, alignment);
+    if (!mem)
+      break;
+    res->deallocate(mem, size, alignment);
+  }
+
+  std::cout << "Preallocating " << (size >> 20) << " MiB of pinned memory" << std::endl;
+
+  try {
+    // Try to preallocate the pool so we're able to get the requested amount
+    mm::PreallocatePinnedMemory(size);
+  } catch (const std::bad_alloc &) {
+    GTEST_SKIP() << "Not enough memory to test pool preallocation.";
+  }
+
+  void *mem = pool->try_allocate_from_free(size, alignment);
+  EXPECT_NE(mem, nullptr) << "Preallocation succeeded, so we should be able to get the "
+                             "requested amount of memory from the pool.";
+
+  res->deallocate(mem, size, alignment);
+
+  mm::ReleaseUnusedMemory();
+}
+
+static void TestPreallocateDeviceMemory(bool multigpu) {
+  int device_id = multigpu ? 1 : 0;
+  DeviceGuard dg(device_id);
+  mm::ReleaseUnusedMemory();  // release any unused memory to check that we're really preallocating
+
+  CUDA_CALL(cudaSetDevice(0));
+
+  auto *res = mm::GetDefaultDeviceResource(device_id);
+  auto *pool = GetPoolInterface(res);
+  if (!pool)
+    GTEST_SKIP() << "No memory pool in use - cannot test pool preallocation.";
+
+  size_t size = 64_uz << 20;  // 64 MiB
+  size_t alignment = alignof(std::max_align_t);  // default alignment
+  // Try to get increasing amount of memory from the pool until we can't
+  for (;; size <<= 1) {
+    void *mem = pool->try_allocate_from_free(size, alignment);
+    if (!mem)
+      break;
+    res->deallocate(mem, size, alignment);
+  }
+
+  std::cout << "Preallocating " << (size >> 20) << " MiB of memory on device "
+            << device_id << std::endl;
+
+  try {
+    // Try to preallocate the pool so we're able to get the requested amount
+    mm::PreallocateDeviceMemory(size, device_id);
+  } catch (const std::bad_alloc &) {
+    GTEST_SKIP() << "Not enough memory to test pool preallocation.";
+  }
+
+  void *mem = pool->try_allocate_from_free(size, alignment);
+  EXPECT_NE(mem, nullptr) << "Preallocation succeeded, so we should be able to get the "
+                             "requested amount of memory from the pool.";
+
+  auto *vm_res = GetVMMDefaultResource(device_id);
+  int prev_unmaps = 0;
+  if (vm_res) {
+    prev_unmaps = vm_res->get_stat().total_unmaps;
+    std::cout << "Unmaps (pre)  " << prev_unmaps << std::endl;
+  }
+
+  res->deallocate(mem, size, alignment);
+
+  mm::ReleaseUnusedMemory();
+
+  // Some memory should have been deallocated in ReleaseUnusedMemory, so now the
+  // allocation from the pool should fail again.
+  mem = pool->try_allocate_from_free(size, alignment);
+  EXPECT_EQ(mem, nullptr);
+  if (mem)
+    res->deallocate(mem, size, alignment);
+
+  if (vm_res) {
+    // some unmapping should have occurred
+    EXPECT_GT(vm_res->get_stat().total_unmaps, prev_unmaps);
+    std::cout << "Unmaps (post) " << vm_res->get_stat().total_unmaps << std::endl;
+  }
+}
+
+TEST(MMDefaultResource, PreallocateDeviceMemory) {
+  for (int i = 0; i < 20; i++) {
+    cout << "Iteration " << i << endl;
+    TestPreallocateDeviceMemory(false);
+    if (HasFailure())
+      break;
+  }
+}
+
+TEST(MMDefaultResource, PreallocateDeviceMemory_MultiGPU) {
+  int num_devices = 0;
+  CUDA_CALL(cudaGetDeviceCount(&num_devices));
+  if (num_devices >= 2) {
+    TestPreallocateDeviceMemory(true);
+  } else {
+    GTEST_SKIP() << "At least 2 devices needed for the test\n";
+  }
+}
+
 }  // namespace test
 }  // namespace mm
 }  // namespace dali
diff --git a/include/dali/core/mm/cuda_vm_resource.h b/include/dali/core/mm/cuda_vm_resource.h
index 92955693..ad1e2f2c 100644
--- a/include/dali/core/mm/cuda_vm_resource.h
+++ b/include/dali/core/mm/cuda_vm_resource.h
@@ -74,7 +74,10 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
   struct Stat {
     int allocated_blocks;
     int peak_allocated_blocks;
+    int va_ranges;
+    int peak_va_ranges;
     size_t allocated_va;
+    size_t peak_va;
     size_t curr_allocated;
     size_t peak_allocated;
     size_t curr_free;
@@ -106,6 +109,9 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
   void dump_stats(std::ostream &os) {
     print(os, "cuda_vm_resource stat dump:",
       "\ntotal VM size:         ", stat_.allocated_va,
+      "\npeak VM size:          ", stat_.peak_va,
+      "\n# VM ranges:           ", stat_.va_ranges,
+      "\npeak # VM ranges:      ", stat_.peak_va_ranges,
       "\ncurrently allocated:   ", stat_.curr_allocated,
       "\npeak allocated:        ", stat_.peak_allocated,
       "\nallocated_blocks:      ", stat_.allocated_blocks,
@@ -114,7 +120,8 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
       "\ntotal allocations:     ", stat_.total_allocations,
       "\ntotal deallocations:   ", stat_.total_deallocations,
       "\ntotal unmapping:       ", stat_.total_unmaps,
-      "\nfree pool size:        ", stat_.curr_free);
+      "\nfree pool size:        ", stat_.curr_free,
+      "\n");
   }
 
   void dbg_dump(std::ostream &os) {
@@ -142,11 +149,177 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
     }
   }
 
+  /**
+   * @brief Releases unused physical blocks
+   *
+   * Releases physical blocks that are currently allocated, but fully available.
+   */
+  void release_unused() {
+    do_release_unused(false);
+  }
+
+ protected:
+  std::pair<size_t, size_t> do_release_unused(bool release_va) {
+    size_t mem_freed = 0, va_freed = 0;
+    std::vector<cuvm::CUMem> blocks_to_free;
+    {
+      std::unique_lock pool_guard(pool_lock_, std::try_to_lock);
+      std::unique_lock mem_guard(mem_lock_, std::try_to_lock);
+
+      ptrdiff_t num_blocks_to_free = 0;
+      for (auto &region : va_regions_) {
+        ptrdiff_t start = 0, end = 0;
+        for (;; start = end + 1) {
+          start = region.available.find(true, start);
+          end = region.available.find(false, start + 1);
+          if (end <= start)
+            break;
+          num_blocks_to_free += end - start;
+        }
+      }
+      blocks_to_free.reserve(num_blocks_to_free);
+
+      [&]() noexcept {  // this code mustn't throw!
+        for (auto &region : va_regions_) {
+          ptrdiff_t start = 0, end = 0;
+          for (;; start = end + 1) {
+            start = region.available.find(true, start);
+            end = region.available.find(false, start + 1);
+            if (end <= start)
+              break;
+            auto *start_ptr = region.block_ptr<char>(start);
+            auto *end_ptr = region.block_ptr<char>(end);
+            for (ptrdiff_t i = start; i < end; i++) {
+              blocks_to_free.push_back(region.unmap_block(i));
+              stat_.total_unmaps++;
+              mem_freed += block_size_;
+            }
+            free_mapped_.get_specific_block(start_ptr, end_ptr);
+          }
+        }
+      }();
+      assert(static_cast<ptrdiff_t>(blocks_to_free.size()) == num_blocks_to_free);
+      stat_.allocated_blocks -= blocks_to_free.size();
+    }
+    blocks_to_free.clear();  // free the physical memory
+    if (release_va)
+      va_freed = release_unused_va();
+    return { mem_freed, va_freed };
+  }
+
  protected:
   void do_deallocate(void *ptr, size_t size, size_t alignment) override {
     deallocate_impl(ptr, size, alignment, true);
   }
 
+  int find_va_region(CUdeviceptr ptr) {
+    for (size_t i = 0; i < va_regions_.size(); i++)
+      if (va_regions_[i].address_range.contains(ptr))
+        return i;
+    return -1;
+  }
+
+  size_t release_unused_va() {
+    struct RangeDesc {
+      int range_idx, region_idx;
+      int start_block, end_block;
+    };
+    std::vector<cuvm::CUMem> blocks_to_free;
+    std::vector<cuvm::CUMemAddressRange> va_ranges_to_free;
+    std::vector<RangeDesc> ranges_to_free;
+    blocks_to_free.reserve(stat_.allocated_blocks);
+    va_ranges_to_free.reserve(va_ranges_.size());
+    ranges_to_free.reserve(va_ranges_.size());
+    size_t va_freed = 0;
+
+    {
+      std::unique_lock pool_guard(pool_lock_, std::try_to_lock);
+      std::unique_lock mem_guard(mem_lock_, std::try_to_lock);
+
+      // the whole block below musn't throw or it'll leave the resource in an inconsistent state
+      [&]() noexcept {
+        for (int i = 0, n = va_ranges_.size(); i < n; i++) {
+          auto start = va_ranges_[i].ptr();
+          auto end = va_ranges_[i].end();
+          if (free_va_.contains(reinterpret_cast<void*>(start), reinterpret_cast<void*>(end))) {
+            int region_idx = find_va_region(start);
+            assert(region_idx >= 0);
+            va_region &region = va_regions_[region_idx];
+
+            ptrdiff_t offset = start - region.address_range.ptr();
+            assert(offset % block_size_ == 0);
+            int num_blocks = (end - start) / block_size_;
+            int start_block = offset / block_size_;  // index of the first block
+            int end_block = start_block + num_blocks;  // index of the last block
+
+            int avail = region.available.find(true, start_block);
+            while (avail < end_block) {
+              int end_avail = std::min<int>(region.available.find(false, avail + 1), end_block);
+              for (int b = avail; b < end_avail; b++) {
+                blocks_to_free.push_back(region.unmap_block(b));
+                stat_.total_unmaps++;
+              }
+              auto *start_ptr = region.block_ptr<char>(avail);
+              auto *end_ptr = region.block_ptr<char>(end_avail);
+              free_mapped_.get_specific_block(start_ptr, end_ptr);
+
+              if (end_avail == end_block)
+                break;
+              avail = region.available.find(true, end_avail + 1);
+            }
+
+            ranges_to_free.push_back({ i, region_idx, start_block, end_block });
+          }
+        }
+        stat_.allocated_blocks -= blocks_to_free.size();
+      }();
+
+      for (const RangeDesc &rd : ranges_to_free) {
+        auto &region = va_regions_[rd.region_idx];
+
+        // We split the va_region as follows:
+
+        // |<--------------- old region ---------------------->|
+        // |<---region-->|<--va_range_to_remove-->|<---tail--->|
+        //               ^                        ^
+        // start_block---^                        ^---- end_block
+
+        // If start_block is 0 then `region` will become empty and we can either
+        // overwrite it with tail (if not empty) or remove it altogether
+
+        va_region tail = region.split(rd.end_block);
+        region.resize(rd.start_block);
+
+        auto *start_ptr = region.block_ptr<char>(rd.start_block);
+        auto *end_ptr = region.block_ptr<char>(rd.end_block);
+        ptrdiff_t range_size = (end_ptr - start_ptr);
+
+        if (region.empty()) {
+          if (tail.empty())
+            va_regions_.erase(va_regions_.begin() + rd.region_idx);
+          else
+            region = std::move(tail);
+        } else if (!tail.empty()) {
+          va_regions_.push_back(std::move(tail));
+        }
+
+        va_ranges_to_free.push_back(std::move(va_ranges_[rd.range_idx]));
+        va_ranges_.erase(va_ranges_.begin() + rd.range_idx);
+        stat_.allocated_va -= range_size;
+        va_freed += range_size;
+        void *p = free_va_.get_specific_block(start_ptr, end_ptr);
+        assert(p != nullptr);
+        (void)p;  // for non-debug builds
+      }
+
+      stat_.va_ranges -= va_ranges_to_free.size();
+    }
+
+    blocks_to_free.clear();
+    va_ranges_to_free.clear();
+    return va_freed;
+  }
+
   void *do_allocate(size_t size, size_t alignment) override {
     if (size == 0)
       return nullptr;
@@ -227,6 +400,10 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
       purge();
     }
 
+    bool empty() const {
+      return num_blocks() == 0;
+    }
+
     int num_blocks() const {
       return mapping.size();
     }
@@ -271,7 +448,7 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
       assert(available[block_idx]);
       cuvm::Unmap(block_dptr(block_idx), block_size);
       cuvm::CUMem mem({mapping[block_idx], block_size });
-      mapping[block_idx] = 0;
+      mapping[block_idx] = {};
       mapped[block_idx] = false;
       available[block_idx] = false;
       available_blocks--;
@@ -279,6 +456,11 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
       return mem;
     }
 
+    /**
+     * @brief Adds the contents of `other` at the end of this region.
+     *
+     * @note This function operates purely on metadata and doesn't affect the process memory map.
+     */
     void append(va_region &&other) {
       assert(address_range.end() == other.address_range.ptr());
       assert(block_size == other.block_size);
@@ -303,6 +485,53 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
       assert(mapping.size() == available.size());
     }
 
+    /**
+     * @brief Trims the current region to `tail_start` blocks and returns the rest as a new region.
+     *
+     * @note This function operates purely on metadata and doesn't affect the process memory map.
+     *
+     * @param tail_start  The index of the first block to be moved to `tail`
+     * @return va_region  The region starting at `tail_start`.
+     */
+    va_region split(int tail_start) {
+      if (tail_start == 0) {
+        va_region ret = std::move(*this);
+        available_blocks = 0;
+        address_range = {};
+        return ret;
+      } else if (tail_start == num_blocks()) {
+        return va_region({}, block_size);
+      }
+      int tail_blocks = num_blocks() - tail_start;
+      cuvm::CUAddressRange tail_range(block_dptr(tail_start), tail_blocks * block_size);
+      va_region tail(tail_range, block_size);
+      int n = num_blocks();
+      for (int src = tail_start, dst = 0; src < n; src++, dst++) {
+        tail.mapping[dst] = std::move(available[src]);
+        tail.mapped[dst] = mapped[src];
+
+        bool avail = available[src];
+        tail.available[dst] = avail;
+        if (avail) {  // update the available block count
+          tail.available_blocks++;
+          available_blocks--;
+        }
+      }
+      resize(tail_start);
+      return tail;
+    }
+
+    /**
+     * @brief Changes the size of the region.
+     *
+     * If the new size is smaller than the old one and there are any blocks mapped at indices
+     * that would become out of range, they are unmapped and deallocated.
+     *
+     * @note This function may change the process memory map.
+     *
+     * @note This function affects just the region and doesn't adjust the free_va / free_mapped
+     *       in the enclosing VM resource. These need to be adjusted by the caller.
+     */
     void resize(int new_num_blocks) {
       if (new_num_blocks < num_blocks()) {
         int no_longer_in_range = 0;
@@ -406,7 +635,10 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
    *
    * The function hints the driver to use a distinct address space for each device in the
    * attempt to have a contiguous address spaces for each device. Currently, the spacing
-   * between device VA spaces is 1 TiB and initial VA size for each device is 4 GiB.
+   * between device VA spaces is 1 TiB and initial VA size for each device is double the physical
+   * size.
+   * On platforms that restrict VA size, if the reservation, fails there are more attempts to
+   * allocate the size that's large enough to accommodate the requested size.
    */
   void va_allocate(size_t min_size) {
     size_t va_size = std::max(next_pow2(min_size), initial_va_size_);
@@ -421,7 +653,7 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
 
     if (va_regions_.empty()) {
       // Calculate the alignment for the initial allocations for this device - we start from
-      // 4 TiB nad go down.
+      // 4 TiB and go down.
       // The address hint is not important.
       hints = {
         { 0_zu, 1_zu << 42 },
@@ -442,23 +674,48 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
       };
     }
 
-    // Try to allocate at hinted locations...
-    for (auto hint : hints) {
-      try {
-        va = cuvm::CUMemAddressRange::Reserve(va_size, hint.alignment, hint.address);
-        break;
-      } catch (const CUDAError &) {
-      } catch (const std::bad_alloc &) {}
+    while (!va) {
+      // Try to allocate at hinted locations...
+      for (auto hint : hints) {
+        try {
+          va = cuvm::CUMemAddressRange::Reserve(va_size, hint.alignment, hint.address);
+          break;
+        } catch (const CUDAError &) {
+        } catch (const std::bad_alloc &) {}
+      }
+      if (!va) {
+        // ...hint failed - allocate anywhere, just align to block_size_
+        auto on_error = [&](auto &exception) {
+          size_t next_va_size;
+          next_va_size = std::max(align_up(min_size, block_size_), va_size >> 1);
+          if (next_va_size == va_size) {  // we're already as low as we can - rethrow
+            if (!release_unused_va())
+              throw exception;
+          }
+          va_size = next_va_size;
+        };
+
+        try {
+          va = cuvm::CUMemAddressRange::Reserve(va_size, block_size_, 0);
+          break;
+        } catch (const CUDAError &e) {
+          on_error(e);
+        } catch (const std::bad_alloc &e) {
+          on_error(e);
+        }
+      }
     }
-    if (!va)  // ...hint failed - allocate anywhere, just align to block_size_
-      va = cuvm::CUMemAddressRange::Reserve(va_size, block_size_, 0);
+
+
+    if (va_size < initial_va_size_)
+        initial_va_size_ = va_size;
 
     if (!mm::detail::is_aligned(detail::u2ptr(va.ptr()), block_size_))
       throw std::logic_error("The VA region is not aligned to block size!\n"
         "This should never happen.");
 
     va_ranges_.push_back(std::move(va));
-    va_add_region(va_ranges_.back());
+    va_add_range(va_ranges_.back());
     stat_va_add(va_size);
   }
 
@@ -466,7 +723,7 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
    * @brief Add a memory region that spans the given VA range and merge it with adjacent
    *        regions, if found.
    */
-  void va_add_region(cuvm::CUAddressRange va) {
+  void va_add_range(cuvm::CUAddressRange va) {
     // Try to merge regions
     // 1. Find preceding region
     va_region *region = nullptr;
@@ -754,6 +1011,11 @@ class cuda_vm_resource : public memory_resource<memory_kind::device> {
 
   void stat_va_add(size_t size) {
     stat_.allocated_va += size;
+    if (stat_.allocated_va > stat_.peak_va)
+      stat_.peak_va = stat_.allocated_va;
+    stat_.va_ranges++;
+    if (stat_.va_ranges > stat_.peak_va_ranges)
+      stat_.peak_va_ranges = stat_.va_ranges;
   }
 };
 
-- 
2.17.1

